---
title: "Interpretable Machine Learning PD5"
author: "Daniel Ponikowski"
date: "14 kwietnia 2019"
output: pdf_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(caret)
library(ggplot2)
library(gtools)
library(rpart.plot)
library(rpart)
library(readstata13)
library(reshape2)
```

## Wybrane zmienne :

    1. ppwork - aktualny status zatrudnienia
    2. w6_q20 - czy obecnie mieszkasz z partnerem?
    3. Q21A_Year - w ktorym roku pierwszy raz spotkales partnera?
    4. ppage - wiek


## Wczytanie danych:
```{R message=FALSE, warning=FALSE}
data <- read.dta13(file = "../PD1/HCMST 2017 fresh sample for public sharing draft v1.1.dta")
df <- data[,c("S1","ppwork","w6_q19","Q21A_Year","ppage")]
df <- df %>% mutate(Q21A_Year = as.numeric(as.character(Q21A_Year))
                    ,ppwork = factor(ppwork)
                    ,w6_q19 = factor(w6_q19)
                    ,ppage = as.numeric(ppage)
                    ,S1= factor(S1)) %>%
  na.omit() %>% unique() %>% as.data.frame()
row.names(df) <- 1:nrow(df)
```

## Modele

Uzyje dwoch modeli, rozniacych sie struktura. Pierwszym modelem, bedzie las losowy uzywany w poprzednich pracach domowych, natomiast drugim modelem bedzie regresja logistyczna.
```{R message=FALSE, warning=FALSE}
RF <- readRDS("../PD3/randomForestPD1.rds")
logit <- train(S1~.,df,"glmnet",family = "binomial")
```

## PD5

```{r}
Accuracy <- function(y_pred,y){
  mat <- table(y_pred,y)
  sum(diag(mat))/sum(mat)
}
variable_importance <- function(model,data_X,Y){
  importance <- list()
  data <- data_X
  basic_accuracy <- Accuracy(predict(model,data_X),Y)
  for (zmienna in colnames(data_X)){
    data[[zmienna]] <- sample(data[[zmienna]])
    accuracy_zmienna <- Accuracy(predict(model,data),Y)
    importance[[zmienna]] <- basic_accuracy - accuracy_zmienna
    data <- data_X
  }
  importance <- unlist(importance)
  importance
}

variable_importance_plot <- function(lista_modeli,data_X,Y){
  wyniki <- lapply(lista_modeli,function(x) variable_importance(model = x,data_X = data_X,
                                                                Y = Y)) %>%
    unlist() %>% matrix(nrow = ncol(data_X))
  colnames(wyniki) <- lapply(lista_modeli,function(x) x$method)
  rownames(wyniki) <- colnames(data_X)
  wyniki <- melt(wyniki,varnames = c("zmienne","modele")) %>%
    mutate(value = round(value,4))

  ggplot(wyniki,aes(x = zmienne,y = value, fill = modele)) +
    geom_bar(stat = "identity", position=position_dodge()) +
    geom_text(aes(label=value), position = position_dodge(0.9),vjust = 1.4,
              color="black", size=3.5) +
    theme_minimal() + scale_fill_brewer(palette="Paired") +
    ggtitle(label = "Variable importance") +
    theme(plot.title = element_text(hjust = 0.5)) +
    ylab(label = "acc_mod - sample_acc")
  }
```


## Waznosc zmiennych dla modeli:
```{r}
variable_importance_plot(list(RF,logit),df[,-1],df$S1)  
```

## PD plot

Przedstawie PD ploty zmiennej ppage (wiek), ktora dla lasu losowego jest wazna zmienna (druga co do waznosci), a regresja logistyczna calkowicie ignoruje ta zmienna (roznica pomiedzy accuracy bazowym i accuracy po zmianie tej zmiennej jest bardzo sobie bliskie).

```{r}
PD <- function(model,data_X,zmienna){
  result <- data.frame(zmienna = min(data_X[[zmienna]]):max(data_X[[zmienna]]))
  for (i in 1:nrow(data_X)){
    n <- nrow(result)
    Q21A_Year <- rep(data_X$Q21A_Year[i], n)
    ppwork <- rep(data_X$ppwork[i], n)
    w6_q19 <- rep(data_X$w6_q19[i], n)
    df <- data.frame(ppage = result$zmienna, ppwork = ppwork,w6_q19=w6_q19,
                     Q21A_Year=Q21A_Year)
    result[[i+1]] <- predict(model,df,"prob")[,1]
  }
  ppage <- result$zmienna
  result$zmienna <- NULL
  prob <- as.data.frame(cbind(ppage,prob_of_maried = apply(result,1,mean)))
  prob
}

PD_plot <- function(lista_modeli,data_X,zmienna){
  wynik <- lapply(lista_modeli,function(x) PD(model = x,data_X = data_X
                                              ,zmienna = zmienna)) %>%
    bind_rows(.id = "model")
  colnames(wynik) <- c("model","zmienna","prob_of_maried")
  ggplot(wynik,aes(x=zmienna,y=prob_of_maried)) +
    geom_line(aes(colour = model)) +
    ggtitle(paste("PD plot zmiennej",zmienna)) +
    theme(plot.title = element_text(hjust = 0.5)) + xlab(zmienna)
  }
```

## PD plot
```{r echo=FALSE}
PD_plot(list(rf = RF,logit = logit),data_X = df,zmienna = "ppage")
```

## Komentarz

Wp³yw zmiennej *ppage* (wiek) nie jest wychwytywany przez model regresji logistycznej, moze to wynikac z ograniczenia modelu (zaloenie liniowej zaleznosci), las losowy jest w stanie wychwyciæ zale¿noœci nieliniowe. Roznica w skrajnych punktach PD plotu, dla regresji liniowej, wynosi ok. 0.02, wiec niewiele. PD plot dla lasu losowego pokazuje wieksze wahania, mozemy to interpretowac nastepujaco, dla lasu losowego odpowiedz modelu zalezy od wartosci tej zmiennej, gdzie w przypadku regresji liniowej wartosc tej zmiennej w niewielkim stopniu wplywa na odpowiedz modelu.

Najwazniejsza zmienna dla obu modeli okazala sie zmienna *Q21A_Year* (w ktorym roku pierwszy raz spotkales partnera?).




